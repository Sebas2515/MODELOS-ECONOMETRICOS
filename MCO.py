
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm
import statsmodels.stats.api as sms
from statsmodels.stats.outliers_influence import variance_inflation_factor
import matplotlib.pyplot as plt
from pathlib import Path
from tabulate import tabulate


################################################################################
#                           MCO
################################################################################

# 1Ô∏è‚É£ Cargar datos
path = Path('DATA/bd-subnacionales.xlsx')
df = pd.read_excel(path)

# 2Ô∏è‚É£ Limpiar nombres de columnas (quitar espacios al inicio/final y saltos de l√≠nea)
df.columns = df.columns.str.strip().str.replace('\n','').str.replace('\r','')

print(df)

################################################################################
# PASO 3: TEST DE ESTACIONARIEDAD (DICKEY-FULLER AUMENTADO)
################################################################################

def adf_test(series, name=''):
    """Realiza el test de Dickey-Fuller Aumentado en una serie temporal."""
    result = adfuller(series.dropna())
    print(f'--- Test de Estacionariedad para: {name} ---')
    print(f'ADF Statistic: {result[0]:.4f}')
    print(f'p-value: {result[1]:.4f}')
    if result[1] <= 0.05:
        print("Resultado: Evidencia fuerte contra la hip√≥tesis nula (H0), la serie es estacionaria.\n")
    else:
        print("Resultado: Evidencia d√©bil contra H0, la serie tiene una ra√≠z unitaria y es no-estacionaria.\n")

# =====================================
# 3.1 Verificando Estacionariedad SOLO en las series logar√≠tmicas
# =====================================
print("\n--- 3. Verificando Estacionariedad de las series logar√≠tmicas ---")

# Filtrar columnas que contienen 'ln_' en su nombre
cols_log = [col for col in df.columns if 'ln_' in col]

# Crear DataFrame solo con las columnas logar√≠tmicas
df_log = df[cols_log]

# Aplicar el test ADF a las series en logaritmos
for name, column in df_log.items():
    adf_test(column, name=name)

# =====================================
# 3.2 Verificando Estacionariedad en las series logar√≠tmicas diferenciadas
# =====================================
print("\n--- 4. Verificando Estacionariedad de las series logar√≠tmicas en primera diferencia ---")

# Diferenciar solo las columnas logar√≠tmicas
df_log_diff = df_log.diff().dropna()

# Aplicar el test ADF a las series logar√≠tmicas diferenciadas
for name, column in df_log_diff.items():
    adf_test(column, name=f'{name}_diff')































"""
# 3Ô∏è‚É£ Renombrar columnas para trabajar m√°s f√°cil
df = df.rename(columns={
    'TASA % EJECUCI√ìN PRESUPUESTAL (avance)': 'Ejecucion',
    'Capacidad recaudatoria total': 'Capacidad_recaudatoria',
    'Recursos directamente recaudados': 'RDR'
})

# Aplicar logaritmo natural (ln) a las variables positivas
df['ln_PIM'] = np.log(df['PIM'])
df['ln_Capacidad_recaudatoria'] = np.log(df['Capacidad_recaudatoria'])


# 3Ô∏è‚É£ Definir variables
Y = df['Ejecucion']  # variable dependiente
X = df[['ln_PIM','ln_Capacidad_recaudatoria','RDR']]  # variables explicativas
X = sm.add_constant(X)  # agregar intercepto

# 4Ô∏è‚É£ Ajustar modelo MCO simple

model = sm.OLS(Y, X).fit()
residuos = model.resid
"""
"""
# 6Ô∏è‚É£ Crear tabla de coeficientes con intervalos de confianza
coef_table = pd.DataFrame({
    'Coeficiente': model.params,
    'Error Std': model.bse,
    't-stat': model.tvalues,
    'p-value': model.pvalues,
    'IC 0.025': model.conf_int()[0],
    'IC 0.975': model.conf_int()[1]
})

# 7Ô∏è‚É£ Crear tabla resumen general del modelo
summary_table = pd.DataFrame({
    'Estad√≠stico': ['R-squared', 'Adj. R-squared', 'F-statistic', 'Prob (F-statistic)',
                    'No. Observations', 'Log-Likelihood', 'AIC', 'BIC', 'Df Residuals', 'Df Model', 'Covariance Type'],
    'Valor': [model.rsquared, model.rsquared_adj, model.fvalue, model.f_pvalue,
              int(model.nobs), model.llf, model.aic, model.bic, model.df_resid, model.df_model, 'nonrobust']
})

# 8Ô∏è‚É£ Mostrar todo con tabulate
print("\n=== Resumen General del Modelo ===")
print(tabulate(summary_table, headers='keys', tablefmt='fancy_grid', floatfmt=".4f"))

print("\n=== Coeficientes del Modelo ===")
print(tabulate(coef_table, headers='keys', tablefmt='fancy_grid', floatfmt=".6f"))

"""
########################## PRUEBA DE CORRELACI√ìN ##########################

"""
# 5Ô∏è‚É£ Seleccionar variables que quieres correlacionar
cols = ['Ejecucion', 'RDR', 'ln_PIM', 'ln_Capacidad_recaudatoria']

# 6Ô∏è‚É£ Calcular matriz de correlaciones
corr_matrix = df[cols].corr()

# 7Ô∏è‚É£ Mostrar matriz en consola
print("\n=== Matriz de Correlaciones ===")
print(corr_matrix.round(3))

# 8Ô∏è‚É£ Visualizar matriz con heatmap
plt.figure(figsize=(8,6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap de Correlaciones')
plt.show()
"""

########################## PRUEBA DE AUTOCORRELACION (DURBIN_WATSON) ##########################
"""
from statsmodels.stats.stattools import durbin_watson

dw = durbin_watson(model.resid)
print('Durbin-Watson:', dw)
"""
# Interpretaci√≥n:
# DW ‚âà 2.0: No hay autocorrelaci√≥n (ideal).
# DW < 2.0: Posible autocorrelaci√≥n positiva.
# DW > 2.0: Posible autocorrelaci√≥n negativa.

########################## PRUEBA DE HETEROCEDASTICIDAD (TEST DE WHITE) ##########################
"""
from statsmodels.stats.diagnostic import het_white
# Test de White
white_test = het_white(model.resid, model.model.exog)

# Etiquetas
labels = ['LM stat', 'LM p-value', 'F-stat', 'F p-value']

# Crear un DataFrame para mostrarlo bonito
white_df = pd.DataFrame([white_test], columns=labels)

print("üìä Resultado del Test de White (Heterocedasticidad)")
print(white_df)

# H0 (Hip√≥tesis nula): La varianza de los errores es constante (homocedasticidad)
# H1 (Hip√≥tesis alternativa): La varianza de los errores no es constante (heterocedasticidad)

# Nota: Si el p-value del test es mayor que el nivel de significancia (por ejemplo, 0.05),
# no se rechaza H0, lo que indica que no hay evidencia significativa de heterocedasticidad.
"""

########################## PRUEBA DE MULTICOLINEALIDAD VIF ##########################
"""
#Calcular VIF
vif_data = pd.DataFrame()
vif_data['Variable'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)
"""


########################## PRUEBA DE NORMALIDAD EN LOS RESIDUOS (JARQUE - BERA) ##########################
"""
from statsmodels.stats.stattools import jarque_bera

jb_stat, jb_pvalue, skew, kurtosis = jarque_bera(residuos)

print("Jarque-Bera Test")
print("JB estad√≠stico:", jb_stat)
print("p-value:", jb_pvalue)
print("Skew:", skew)
print("Kurtosis:", kurtosis)

if jb_pvalue > 0.05:
    print("No se rechaza H0: los residuos se distribuyen normalmente")
else:
    print("Se rechaza H0: los residuos no son normales")
"""

########################## PRUEBA DE ESTABILIDAD ESTRUCTURAL (JARQUE - BERA) ##########################
"""
from scipy import stats

# Supongamos que 'df' tiene tu data de 2014-2024
# Dividir en dos subperiodos
df1 = df[df['A√ëO'] <= 2018]
df2 = df[df['A√ëO'] > 2018]

Y1 = df1['Ejecucion'] 
X1 = sm.add_constant(df1[['ln_PIM', 'ln_Capacidad_recaudatoria', 'RDR']])

Y2 = df2['Ejecucion'] 
X2 = sm.add_constant(df2[['ln_PIM', 'ln_Capacidad_recaudatoria', 'RDR']])

# Ajustar los modelos
model_full = sm.OLS(Y, X).fit()
model1 = sm.OLS(Y1, X1).fit()
model2 = sm.OLS(Y2, X2).fit()

# N√∫mero de observaciones y par√°metros
n1 = len(Y1)
n2 = len(Y2)
k = X.shape[1]  # n√∫mero de par√°metros incluyendo constante

# Sum of Squared Residuals
SSR_full = sum(model_full.resid ** 2)
SSR1 = sum(model1.resid ** 2)
SSR2 = sum(model2.resid ** 2)

# Estad√≠stico F de Chow
numerador = (SSR_full - (SSR1 + SSR2)) / k
denominador = (SSR1 + SSR2) / (n1 + n2 - 2*k)
F_chow = numerador / denominador

# p-value
p_value = 1 - stats.f.cdf(F_chow, k, n1 + n2 - 2*k)

print("üìä Test de Chow (estabilidad estructural)")
print("F estad√≠stico:", F_chow)
print("p-value:", p_value)

if p_value < 0.05:
    print("Se rechaza H0: hay evidencia de cambio estructural")
else:
    print("No se rechaza H0: el modelo es estructuralmente estable")
"""

"""
# --- 1. Autocorrelaci√≥n de los residuos (Durbin-Watson) ---
print("\n--- PRUEBA DE AUTOCORRELACI√ìN (Durbin-Watson) ---")
dw_stats = durbin_watson(model_fitted.resid)
for col, val in zip(model_fitted.names, dw_stats):
    print(f"{col}: {val:.2f} {'‚Üí posible autocorrelaci√≥n' if (val < 1.5 or val > 2.5) else '‚Üí sin autocorrelaci√≥n aparente'}")
"""



















































"""
# 5Ô∏è‚É£ Resultados
print(model.summary())
"""
"""

# 6Ô∏è‚É£ Guardar predicciones
df_model['Y_pred'] = model.predict(X)

# 7Ô∏è‚É£ Gr√°ficos de Ejecucion real vs predicha
outdir = Path('DATA/analisis_outputs')
outdir.mkdir(parents=True, exist_ok=True)

plt.figure()
plt.plot(df_model['A√ëO'], df_model['Ejecucion'], marker='o', label='Real')
plt.plot(df_model['A√ëO'], df_model['Y_pred'], marker='x', label='Predicha')
plt.title('Ejecuci√≥n presupuestal: Real vs Predicha')
plt.xlabel('A√±o')
plt.ylabel('Ejecuci√≥n (0-1)')
plt.legend()
plt.grid(True)
plt.savefig(outdir/'ejecucion_vs_predicha.png')
plt.close()

# 8Ô∏è‚É£ Guardar datos con predicciones
df_model.to_csv(outdir/'df_model_simple.csv', index=False)

print('An√°lisis completo. Resultados y gr√°ficos guardados en:', outdir)
"""






"""
# Convertir Ejecuci√≥n de % a decimal
df['Ejecucion'] = df['Ejecucion'].str.replace('%','').astype(float)/100
"""
"""
# Eliminar filas con datos faltantes en variables que usaremos
df_model = df.dropna(subset=['Ejecucion','PIM','Capacidad_recaudatoria','RDR']).copy()
"""



